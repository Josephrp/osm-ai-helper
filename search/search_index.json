{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenStreetMap AI Helper Blueprint","text":"<p>Blueprints are customizable workflows that help developers build AI applications using open-source tools and models</p> <p>These docs are your companion to mastering the OpenStreetMap AI Helper Blueprint.</p>"},{"location":"#built-with","title":"Built with","text":"<ul> <li>Python 3.10+</li> <li>Ultralytics</li> <li>SAM 2</li> </ul>"},{"location":"#get-started-quickly","title":"\ud83d\ude80 Get Started Quickly","text":""},{"location":"#start-building-the-blueprint-in-minutes","title":"Start building the Blueprint in minutes:","text":"<ul> <li>Getting Started: Quick setup and installation instructions.</li> </ul>"},{"location":"#understand-the-system","title":"\ud83d\udd0d Understand the System","text":""},{"location":"#dive-deeper-into-how-the-blueprint-works","title":"Dive deeper into how the Blueprint works:","text":"<ul> <li>Step-by-Step Guide: A detailed breakdown of the system\u2019s design and workflow.</li> <li>API Reference: Explore the technical details of the core modules.</li> </ul>"},{"location":"#make-it-yours","title":"\ud83c\udfa8 Make It Yours","text":""},{"location":"#customize-the-blueprint-to-fit-your-needs","title":"Customize the Blueprint to fit your needs:","text":"<ul> <li>Customization Guide: Tailor project parameters to fit your needs</li> </ul>"},{"location":"#join-the-community","title":"\ud83c\udf1f Join the Community","text":""},{"location":"#help-shape-the-future-of-blueprints","title":"Help shape the future of Blueprints:","text":"<ul> <li>Future Features &amp; Contributions: Learn about exciting upcoming features and how to contribute to the project.</li> </ul> <p>Have more questions? Reach out to us on Discord and we'll see how we can help:</p> <p></p>"},{"location":"#why-blueprints","title":"Why Blueprints?","text":"<p>Blueprints are more than starter code\u2014they\u2019re your gateway to building AI-powered solutions with confidence. With step-by-step guidance, modular design, and open-source tools, we make AI accessible for developers of all skill levels.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#osm_ai_helper.download_osm","title":"<code>osm_ai_helper.download_osm</code>","text":""},{"location":"api/#osm_ai_helper.download_osm.download_osm","title":"<code>download_osm(area, output_dir, selector, discard=None)</code>","text":"<p>Download OSM elements for the given areas and selector.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>Output directory.</p> required <code>selector</code> <code>str</code> <p>OSM tag to select elements.</p> <p>Example: \"leisure=swimming_pool\"</p> required <code>area</code> <code>str</code> <p>Name of area to download. Can be city, state, country, etc.</p> required <code>discard</code> <code>Optional[dict[str, str]]</code> <p>Discard elements matching any of the given tags. Defaults to None. Example: {\"location\": \"indoor\", \"building\": \"yes\"}</p> <code>None</code> Source code in <code>src/osm_ai_helper/download_osm.py</code> <pre><code>@logger.catch(reraise=True)\ndef download_osm(\n    area: str,\n    output_dir: str,\n    selector: str,\n    discard: Optional[Dict[str, str]] = None,\n):\n    \"\"\"Download OSM elements for the given areas and selector.\n\n    Args:\n        output_dir (str): Output directory.\n        selector (str): OSM tag to select elements.\n\n            Example: [\"leisure=swimming_pool\"](https://wiki.openstreetmap.org/wiki/Tag:leisure%3Dswimming_pool)\n\n        area (str): Name of area to download.\n            Can be city, state, country, etc.\n\n        discard (Optional[dict[str, str]], optional): Discard elements matching\n            any of the given tags.\n            Defaults to None.\n            Example: {\"location\": \"indoor\", \"building\": \"yes\"}\n    \"\"\"\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True, parents=True)\n\n    discard = discard or {}\n\n    logger.info(f\"Downloading osm data for {area}\")\n    elements = [\n        element\n        for element in get_elements(selector, area=area)\n        if all(element.get(\"tags\", {}).get(k) != v for k, v in discard.items())\n    ]\n\n    output_file = output_path / f\"{area}.json\"\n    logger.info(f\"Writing {len(elements)} elements to {output_file}\")\n    output_file.write_text(json.dumps(elements))\n    logger.success(\"Done!\")\n</code></pre>"},{"location":"api/#osm_ai_helper.group_elements_and_download_tiles","title":"<code>osm_ai_helper.group_elements_and_download_tiles</code>","text":""},{"location":"api/#osm_ai_helper.group_elements_and_download_tiles.group_elements_and_download_tiles","title":"<code>group_elements_and_download_tiles(elements_file, output_dir, mapbox_token, zoom=18)</code>","text":"<p>Groups the elements by tile and downloads the satellite image corresponding to the tile.</p> <p>Parameters:</p> Name Type Description Default <code>elements_file</code> <code>str</code> <p>Path to the JSON file containing OSM elements. See download_osm.</p> required <code>output_dir</code> <code>str</code> <p>Output directory. The images and annotations will be saved in this directory. The images will be saved as JPEG files and the annotations as JSON files. The names of the files will be in the format <code>{zoom}_{tile_col}_{tile_row}</code>.</p> required <code>mapbox_token</code> <code>str</code> <p>Mapbox token.</p> required <code>zoom</code> <code>int</code> <p>Zoom level of the tiles to download. See https://docs.mapbox.com/help/glossary/zoom-level/. Defaults to 18.</p> <code>18</code> Source code in <code>src/osm_ai_helper/group_elements_and_download_tiles.py</code> <pre><code>@logger.catch(reraise=True)\ndef group_elements_and_download_tiles(\n    elements_file: str, output_dir: str, mapbox_token: str, zoom: int = 18\n):\n    \"\"\"\n    Groups the elements by tile and downloads the satellite image corresponding to the tile.\n\n    Args:\n        elements_file (str): Path to the JSON file containing OSM elements.\n            See [download_osm][osm_ai_helper.download_osm.download_osm].\n        output_dir (str): Output directory.\n            The images and annotations will be saved in this directory.\n            The images will be saved as JPEG files and the annotations as JSON files.\n            The names of the files will be in the format `{zoom}_{tile_col}_{tile_row}`.\n        mapbox_token (str): [Mapbox](https://console.mapbox.com/) token.\n        zoom (int, optional): Zoom level of the tiles to download.\n            See https://docs.mapbox.com/help/glossary/zoom-level/.\n            Defaults to 18.\n    \"\"\"\n    annotation_path = Path(elements_file)\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True, parents=True)\n\n    elements = json.loads(annotation_path.read_text())\n\n    logger.info(\"Grouping elements by tile\")\n    grouped = group_elements_by_tile(elements, zoom)\n\n    total = len(grouped)\n    n = 0\n    logger.info(\"Downloading tiles and writing annotation\")\n    for (tile_col, tile_row), group in grouped.items():\n        if n % 50 == 0:\n            logger.info(f\"Processed {n}/{total} tiles\")\n        n += 1\n        output_name = f\"{zoom}_{tile_col}_{tile_row}\"\n        image_name = f\"{output_path / output_name}.jpg\"\n        annotation_name = f\"{output_path / output_name}.json\"\n        if not Path(image_name).exists():\n            image = download_tile(zoom, tile_col, tile_row, mapbox_token)\n            image.save(image_name)\n        if not Path(annotation_name).exists():\n            Path(annotation_name).write_text(\n                json.dumps(\n                    {\n                        \"elements\": group,\n                    }\n                )\n            )\n</code></pre>"},{"location":"api/#osm_ai_helper.run_inference","title":"<code>osm_ai_helper.run_inference</code>","text":""},{"location":"api/#osm_ai_helper.run_inference.run_inference","title":"<code>run_inference(yolo_model_file, output_dir, lat_lon, margin=1, sam_model='facebook/sam2-hiera-small', selector='leisure=swimming_pool', zoom=18)</code>","text":"<p>Run inference on a given location.</p> <p>Parameters:</p> Name Type Description Default <code>yolo_model_file</code> <code>str</code> <p>Path to the YOLO model file.</p> required <code>output_dir</code> <code>str</code> <p>Output directory. The images and annotations will be saved in this directory. The images will be saved as PNG files and the annotations as JSON files. The names of the files will be in the format <code>{zoom}_{tile_col}_{tile_row}</code>.</p> required <code>lat_lon</code> <code>Tuple[float, float]</code> <p>Latitude and longitude of the location.</p> required <code>margin</code> <code>int</code> <p>Number of tiles around the location. Defaults to 1.</p> <code>1</code> <code>sam_model</code> <code>str</code> <p>SAM2 model to use. Defaults to \"facebook/sam2-hiera-small\".</p> <code>'facebook/sam2-hiera-small'</code> <code>selector</code> <code>str</code> <p>OpenStreetMap selector. Defaults to \"leisure=swimming_pool\".</p> <code>'leisure=swimming_pool'</code> <code>zoom</code> <code>int</code> <p>Zoom level. Defaults to 18. See https://docs.mapbox.com/help/glossary/zoom-level/.</p> <code>18</code> Source code in <code>src/osm_ai_helper/run_inference.py</code> <pre><code>@logger.catch(reraise=True)\ndef run_inference(\n    yolo_model_file: str,\n    output_dir: str,\n    lat_lon: Tuple[float, float],\n    margin: int = 1,\n    sam_model: str = \"facebook/sam2-hiera-small\",\n    selector: str = \"leisure=swimming_pool\",\n    zoom: int = 18,\n):\n    \"\"\"\n    Run inference on a given location.\n\n    Args:\n        yolo_model_file (str): Path to the [YOLO](https://docs.ultralytics.com/tasks/detect/) model file.\n        output_dir (str): Output directory.\n            The images and annotations will be saved in this directory.\n            The images will be saved as PNG files and the annotations as JSON files.\n            The names of the files will be in the format `{zoom}_{tile_col}_{tile_row}`.\n        lat_lon (Tuple[float, float]): Latitude and longitude of the location.\n        margin (int, optional): Number of tiles around the location.\n            Defaults to 1.\n        sam_model (str, optional): [SAM2](https://github.com/facebookresearch/sam2) model to use.\n            Defaults to \"facebook/sam2-hiera-small\".\n        selector (str, optional): OpenStreetMap selector.\n            Defaults to \"leisure=swimming_pool\".\n        zoom (int, optional): Zoom level.\n            Defaults to 18.\n            See https://docs.mapbox.com/help/glossary/zoom-level/.\n    \"\"\"\n    bbox_predictor = YOLO(yolo_model_file)\n    sam_predictor = SAM2ImagePredictor.from_pretrained(\n        sam_model, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n    )\n\n    bbox = lat_lon_to_bbox(*lat_lon, zoom, margin)\n\n    output_path = Path(output_dir) / f\"{zoom}_{'_'.join(map(str, bbox))}\"\n    output_path.mkdir(exist_ok=True, parents=True)\n\n    logger.info(f\"Downloading elements for {selector} in {bbox}\")\n    elements = get_elements(selector, bbox=bbox)\n    grouped_elements = group_elements_by_tile(elements, zoom)\n    logger.info(f\"Found {len(elements)} elements\")\n\n    logger.info(f\"Downloading all tiles within {bbox}\")\n    stacked_image, stacked_mask = download_stacked_image_and_mask(\n        bbox, grouped_elements, zoom, os.environ[\"MAPBOX_TOKEN\"]\n    )\n    Image.fromarray(stacked_image).save(output_path / \"full_image.png\")\n    Image.fromarray(stacked_mask).save(output_path / \"full_mask.png\")\n\n    logger.info(\"Predicting on stacked image\")\n    # Change to BGR for inference\n    stacked_output = tile_prediction(\n        bbox_predictor, sam_predictor, stacked_image[:, :, ::-1]\n    )\n\n    logger.info(\"Finding existing, new and missed polygons\")\n    existing, new, missed = polygon_evaluation(stacked_mask, stacked_output)\n    logger.info(f\"{len(existing)} exiting, {len(new)} new and {len(missed)} missied.\")\n    logger.info(\"Painting evaluation\")\n    stacked_image_pil = Image.fromarray(stacked_image)\n    painted_img = paint_polygon_evaluation(stacked_image_pil, existing, new, missed)\n    painted_img.save(output_path / \"full_image_painted.png\")\n\n    _, west, north, _ = bbox\n    left_col, top_row = lat_lon_to_tile_col_row(north, west, zoom)\n    top_pixel = top_row * TILE_SIZE\n    left_pixel = left_col * TILE_SIZE\n\n    logger.info(\"Saving new polygons\")\n    for n, polygon in enumerate(new):\n        lon_lat_polygon = pixel_polygon_to_lat_lon_polygon(\n            polygon, top_pixel, left_pixel, zoom\n        )\n\n        with open(f\"{output_path}/{n}.json\", \"w\") as f:\n            json.dump(lon_lat_polygon, f)\n\n        crop_polygon(polygon, painted_img, margin=100).save(\n            f\"{output_path}/{n}_painted.png\"\n        )\n\n        crop_polygon(polygon, stacked_image_pil, margin=100).save(\n            f\"{output_path}/{n}.png\"\n        )\n\n    return output_path, existing, new, missed\n</code></pre>"},{"location":"api/#osm_ai_helper.upload_osm","title":"<code>osm_ai_helper.upload_osm</code>","text":""},{"location":"api/#osm_ai_helper.upload_osm.upload_osm","title":"<code>upload_osm(results_folder, client_id, client_secret)</code>","text":"<p>Upload the results to OpenStreetMap.</p> <p>Parameters:</p> Name Type Description Default <code>results_folder</code> <code>str</code> <p>Folder containing the results. The results should be in the format of <code>*.json</code> files. See <code>run_inference</code>.</p> required <code>client_id</code> <code>str</code> <p>OpenStreetMap Oauth client ID.</p> required <code>client_secret</code> <code>str</code> <p>OpenStreetMap Oauth client secret.</p> required Source code in <code>src/osm_ai_helper/upload_osm.py</code> <pre><code>def upload_osm(results_folder: str, client_id: str, client_secret: str):\n    \"\"\"\n    Upload the results to OpenStreetMap.\n\n    Args:\n        results_folder (str): Folder containing the results.\n            The results should be in the format of `*.json` files.\n            See [`run_inference`][osm_ai_helper.run_inference.run_inference].\n        client_id (str): OpenStreetMap Oauth client ID.\n        client_secret (str): OpenStreetMap Oauth client secret.\n    \"\"\"\n    osm_session = ensure_authorized_session(client_id, client_secret)\n\n    lon_lat_polygons = [\n        json.loads(result.read_text()) for result in Path(results_folder).glob(\"*.json\")\n    ]\n\n    with open_changeset(osm_session) as changeset:\n        for lon_lat_polygon in lon_lat_polygons:\n            upload_polygon(osm_session, lon_lat_polygon, changeset)\n</code></pre>"},{"location":"api/#osm_ai_helper.utils.inference","title":"<code>osm_ai_helper.utils.inference</code>","text":""},{"location":"api/#osm_ai_helper.utils.inference.download_stacked_image_and_mask","title":"<code>download_stacked_image_and_mask(bbox, grouped_elements, zoom, mapbox_token)</code>","text":"<p>Download all tiles within a bounding box and stack them into a single image.</p> <p>All the grouped_elements are painted on the mask.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>tuple</code> <p>Bounding box in the form of (south, west, north, east).</p> required <code>grouped_elements</code> <code>dict</code> <p>OpenStreetMap elements grouped with group_elements_by_tile.</p> required <code>zoom</code> <code>int</code> <p>Zoom level. See https://docs.mapbox.com/help/glossary/zoom-level/.</p> required <code>mapbox_token</code> <code>str</code> <p>Mapbox token. See https://docs.mapbox.com/help/getting-started/access-tokens/.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[ndarray, ndarray]</code> <p>Stacked image and mask.</p> Source code in <code>src/osm_ai_helper/utils/inference.py</code> <pre><code>def download_stacked_image_and_mask(\n    bbox: tuple[float, float, float, float],\n    grouped_elements: dict,\n    zoom: int,\n    mapbox_token: str,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Download all tiles within a bounding box and stack them into a single image.\n\n    All the grouped_elements are painted on the mask.\n\n    Args:\n        bbox (tuple): Bounding box in the form of (south, west, north, east).\n        grouped_elements (dict): OpenStreetMap elements grouped with\n            [group_elements_by_tile][osm_ai_helper.utils.tiles.group_elements_by_tile].\n        zoom (int): Zoom level.\n            See https://docs.mapbox.com/help/glossary/zoom-level/.\n        mapbox_token (str): Mapbox token.\n            See https://docs.mapbox.com/help/getting-started/access-tokens/.\n\n    Returns:\n        tuple: Stacked image and mask.\n    \"\"\"\n    south, west, north, east = bbox\n    left, top = lat_lon_to_tile_col_row(north, west, zoom)\n    right, bottom = lat_lon_to_tile_col_row(south, east, zoom)\n\n    stacked_image = np.zeros(\n        ((right - left) * TILE_SIZE, (bottom - top) * TILE_SIZE, 3), dtype=np.uint8\n    )\n    stacked_mask = np.zeros(\n        ((right - left) * TILE_SIZE, (bottom - top) * TILE_SIZE), dtype=np.uint8\n    )\n\n    for n_col, tile_col in enumerate(range(left, right)):\n        for n_row, tile_row in enumerate(range(top, bottom)):\n            group = grouped_elements[(tile_col, tile_row)]\n\n            img = download_tile(zoom, tile_col, tile_row, mapbox_token)\n\n            mask = grouped_elements_to_mask(group, zoom, tile_col, tile_row)\n\n            stacked_image[\n                n_row * TILE_SIZE : (n_row + 1) * TILE_SIZE,\n                n_col * TILE_SIZE : (n_col + 1) * TILE_SIZE,\n            ] = np.array(img)\n\n            stacked_mask[\n                n_row * TILE_SIZE : (n_row + 1) * TILE_SIZE,\n                n_col * TILE_SIZE : (n_col + 1) * TILE_SIZE,\n            ] = mask\n\n    return stacked_image, stacked_mask\n</code></pre>"},{"location":"api/#osm_ai_helper.utils.inference.tile_prediction","title":"<code>tile_prediction(bbox_predictor, sam_predictor, image, overlap=0.125)</code>","text":"<p>Predict on a large image by splitting it into tiles.</p> <p>Parameters:</p> Name Type Description Default <code>bbox_predictor</code> <code>YOLO</code> <p>YOLO bounding box. See https://docs.ultralytics.com/tasks/detect/.</p> required <code>sam_predictor</code> <code>SAM2ImagePredictor</code> <p>Segment Anything Image Predictor. See https://github.com/facebookresearch/sam2?tab=readme-ov-file#image-prediction.</p> required <code>image</code> <code>ndarray</code> <p>Image to predict on.</p> required <code>overlap</code> <code>float</code> <p>Overlap between tiles. Defaults to 0.125.</p> <code>0.125</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Stacked output.</p> Source code in <code>src/osm_ai_helper/utils/inference.py</code> <pre><code>def tile_prediction(\n    bbox_predictor: YOLO,\n    sam_predictor: SAM2ImagePredictor,\n    image: np.ndarray,\n    overlap: float = 0.125,\n) -&gt; np.ndarray:\n    \"\"\"\n    Predict on a large image by splitting it into tiles.\n\n    Args:\n        bbox_predictor (YOLO): YOLO bounding box.\n            See https://docs.ultralytics.com/tasks/detect/.\n        sam_predictor (SAM2ImagePredictor): Segment Anything Image Predictor.\n            See https://github.com/facebookresearch/sam2?tab=readme-ov-file#image-prediction.\n        image (np.ndarray): Image to predict on.\n        overlap (float): Overlap between tiles.\n            Defaults to 0.125.\n\n    Returns:\n        np.ndarray: Stacked output.\n    \"\"\"\n    stacked_output = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n    for top, left, bottom, right in yield_tile_corners(image, TILE_SIZE, overlap):\n        logger.debug(f\"Predicting {(top, left, bottom, right)}\")\n        tile_image = image[left:right, top:bottom].copy()\n        sam_predictor.set_image(tile_image)\n\n        bbox_result = bbox_predictor.predict(tile_image, verbose=False)\n\n        for bbox in bbox_result:\n            if len(bbox.boxes.xyxy) == 0:\n                continue\n\n            masks, *_ = sam_predictor.predict(\n                point_coords=None,\n                point_labels=None,\n                box=[list(int(x) for x in bbox.boxes.xyxy[0])],\n                multimask_output=False,\n            )\n\n            stacked_output[left:right, top:bottom] += masks[0].astype(np.uint8)\n\n    stacked_output[stacked_output != 0] = 255\n\n    return stacked_output\n</code></pre>"},{"location":"api/#osm_ai_helper.utils.osm","title":"<code>osm_ai_helper.utils.osm</code>","text":""},{"location":"api/#osm_ai_helper.utils.osm.get_area_id","title":"<code>get_area_id(area_name)</code>","text":"<p>Get the Nominatim ID of an area.</p> <p>Uses the Nominatim API.</p> <p>Parameters:</p> Name Type Description Default <code>area_name</code> <code>str</code> <p>The name of the area.</p> required <p>Returns:</p> Type Description <code>Optional[int]</code> <p>Optional[int]: The Nominatim ID of the area.</p> Source code in <code>src/osm_ai_helper/utils/osm.py</code> <pre><code>def get_area_id(area_name: str) -&gt; Optional[int]:\n    \"\"\"\n    Get the Nominatim ID of an area.\n\n    Uses the [Nominatim API](https://nominatim.org/release-docs/develop/api/Search/).\n\n    Args:\n        area_name (str): The name of the area.\n\n    Returns:\n        Optional[int]: The Nominatim ID of the area.\n    \"\"\"\n    response = requests.get(\n        f\"https://nominatim.openstreetmap.org/search?q={area_name}&amp;format=json\",\n        headers={\"User-Agent\": \"Mozilla/5.0\"},\n    )\n    response.raise_for_status()\n    response_json = json.loads(response.content.decode())\n    for area in response_json:\n        osm_type = area.get(\"osm_type\")\n        osm_id = area.get(\"osm_id\")\n        if osm_type == \"way\":\n            return osm_id + 2400000000\n        if osm_type == \"relation\":\n            return osm_id + 3600000000\n</code></pre>"},{"location":"api/#osm_ai_helper.utils.osm.get_elements","title":"<code>get_elements(selector, area=None, bbox=None)</code>","text":"<p>Get elements from OpenStreetMap using the Overpass API.</p> <p>Uses the Overpass API.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>str</code> <p>The selector to use. Example: \"leisure=swimming_pool\"</p> required <code>area</code> <code>Optional[str]</code> <p>The area to search in. Can be city, state, country, etc. Defaults to None.</p> <code>None</code> <code>bbox</code> <code>Optional[Tuple[float, float, float, float]]</code> <p>The bounding box to search in. Defaults to None. Format: https://wiki.openstreetmap.org/wiki/Overpass_API/Language_Guide#The_bounding_box</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>The elements found.</p> Source code in <code>src/osm_ai_helper/utils/osm.py</code> <pre><code>def get_elements(\n    selector: str,\n    area: Optional[str] = None,\n    bbox: Optional[Tuple[float, float, float, float]] = None,\n) -&gt; list[dict]:\n    \"\"\"\n    Get elements from OpenStreetMap using the Overpass API.\n\n    Uses the [Overpass API](https://wiki.openstreetmap.org/wiki/Overpass_API/Language_Guide).\n\n    Args:\n        selector (str): The selector to use.\n            Example: \"leisure=swimming_pool\"\n        area (Optional[str], optional): The area to search in.\n            Can be city, state, country, etc.\n            Defaults to None.\n\n        bbox (Optional[Tuple[float, float, float, float]], optional): The bounding box to search in.\n            Defaults to None.\n            Format: https://wiki.openstreetmap.org/wiki/Overpass_API/Language_Guide#The_bounding_box\n\n    Returns:\n        The elements found.\n    \"\"\"\n    query = \"[out:json];\"\n\n    if area:\n        area_id = get_area_id(area)\n        query += f\"area({area_id})-&gt;.searchArea;(way[{selector}](area.searchArea););\"\n    elif bbox:\n        bbox_str = \",\".join(map(str, bbox))\n        query += f\"(way[{selector}]({bbox_str}););\"\n    else:\n        raise ValueError(\"area or bbox must be provided\")\n\n    query += \" out body geom;\"\n\n    response = requests.get(\n        \"https://overpass-api.de/api/interpreter\",\n        params={\"data\": query},\n        headers={\"User-Agent\": \"Mozilla/5.0\"},\n    )\n    response.raise_for_status()\n    response_json = json.loads(response.content.decode())\n    return response_json[\"elements\"]\n</code></pre>"},{"location":"api/#osm_ai_helper.utils.plots","title":"<code>osm_ai_helper.utils.plots</code>","text":""},{"location":"api/#osm_ai_helper.utils.plots.show_vlm_entry","title":"<code>show_vlm_entry(entry)</code>","text":"<p>Extracts image and points from entry and draws the points.</p> <p>Parameters:</p> Name Type Description Default <code>entry</code> <code>dict</code> <p>Dataset entry generated by <code>convert_to_vlm_dataset</code>. Expected format:</p> <pre><code>entry = {\n    \"messages\": [\n        { \"role\": \"user\",\n        \"content\" : [\n            {\"type\" : \"text\",  \"text\"  : instruction},\n            {\"type\" : \"image\", \"image\" : image} ]\n        },\n        { \"role\" : \"assistant\",\n        \"content\" : [\n            {\"type\" : \"text\",  \"text\"  : str(points)} ]\n        },\n    ]\n}\n</code></pre> required <p>Returns:</p> Name Type Description <code>Image</code> <code>Image</code> <p>Image with points drawn.</p> Source code in <code>src/osm_ai_helper/utils/plots.py</code> <pre><code>def show_vlm_entry(entry) -&gt; Image:\n    \"\"\"\n    Extracts image and points from entry and draws the points.\n\n    Args:\n        entry (dict): Dataset entry generated by `convert_to_vlm_dataset`.\n            Expected format:\n\n            ```py\n            entry = {\n                \"messages\": [\n                    { \"role\": \"user\",\n                    \"content\" : [\n                        {\"type\" : \"text\",  \"text\"  : instruction},\n                        {\"type\" : \"image\", \"image\" : image} ]\n                    },\n                    { \"role\" : \"assistant\",\n                    \"content\" : [\n                        {\"type\" : \"text\",  \"text\"  : str(points)} ]\n                    },\n                ]\n            }\n            ```\n\n    Returns:\n        Image: Image with points drawn.\n    \"\"\"\n    messages = entry[\"messages\"]\n    image = messages[0][\"content\"][1][\"image\"]\n    width, height = image.size\n    points = eval(messages[1][\"content\"][0][\"text\"])\n    draw = ImageDraw.Draw(image)\n\n    for point in points:\n        draw.circle((point[0] * width, point[1] * height), 5, fill=\"red\")\n\n    return image\n</code></pre>"},{"location":"api/#osm_ai_helper.utils.tiles","title":"<code>osm_ai_helper.utils.tiles</code>","text":""},{"location":"api/#osm_ai_helper.utils.tiles.group_elements_by_tile","title":"<code>group_elements_by_tile(elements, zoom)</code>","text":"<p>Broup elements by the tiles they belong to, based on the zoom level.</p> <p>Each MAPBOX tile is a 512x512 pixel image.</p> <p>Parameters:</p> Name Type Description Default <code>elements</code> <code>List[Dict]</code> <p>List of elements from download_osm.</p> required <code>zoom</code> <code>int</code> <p>Zoom level. See https://docs.mapbox.com/help/glossary/zoom-level/.</p> required <p>Returns:</p> Type Description <code>dict[tuple, list[dict]]</code> <p>dict[tuple, list[dict]]: Grouped elements.</p> Source code in <code>src/osm_ai_helper/utils/tiles.py</code> <pre><code>def group_elements_by_tile(elements: List[Dict], zoom: int) -&gt; dict[tuple, list[dict]]:\n    \"\"\"Broup elements by the tiles they belong to, based on the zoom level.\n\n    Each MAPBOX tile is a 512x512 pixel image.\n\n    Args:\n        elements (List[Dict]): List of elements from\n            [download_osm][osm_ai_helper.download_osm.download_osm].\n        zoom (int): Zoom level. See https://docs.mapbox.com/help/glossary/zoom-level/.\n\n    Returns:\n        dict[tuple, list[dict]]: Grouped elements.\n    \"\"\"\n    grouped: dict[tuple, list[dict]] = defaultdict(list)\n\n    for element in elements:\n        pixel_polygon = []\n        for point in element[\"geometry\"]:\n            pixel_point = lat_lon_to_pixel_col_row(point[\"lat\"], point[\"lon\"], zoom)\n            pixel_polygon.append(pixel_point)\n\n        pixel_polygon = np.array(pixel_polygon, dtype=np.int32)\n\n        tiles = map(tuple, np.unique(pixel_polygon // TILE_SIZE, axis=0))\n        for group in tiles:\n            grouped[group].append(element)\n\n    return grouped\n</code></pre>"},{"location":"authorization/","title":"Authorization","text":"<p>In order to use the OpenStreetMap AI Helper Blueprint, there are a couple of authorization accounts you need to set up.</p>"},{"location":"authorization/#mapbox_token","title":"<code>MAPBOX_TOKEN</code>","text":"<p>Used to download the satellite images when creating a dataset and/or running inference.</p> <p>You need to:</p> <ul> <li>Create an account: https://console.mapbox.com/</li> <li>Follow this guide to obtain your Default Public Token.</li> </ul>"},{"location":"authorization/#osm_client_id-and-osm_client_secret","title":"<code>OSM_CLIENT_ID</code> and <code>OSM_CLIENT_SECRET</code>","text":"<p>Used to upload the results after running inference to the OpenStreetMap database.</p> <p>You need to:</p> <ul> <li>Create an account: https://www.openstreetmap.org/user/new</li> <li>Register a new OAuth2 application: https://www.openstreetmap.org/oauth2/applications/new     Grant <code>Modify the map (write_api)</code>.     Set the redirect URL to <code>https://127.0.0.1:8000</code>.</li> <li>Copy and save the <code>Client ID</code> and <code>Client Secret</code>.</li> </ul>"},{"location":"authorization/#hf_token","title":"<code>HF_TOKEN</code>","text":"<p>Only needed if you are Creating a Dataset and/or Finetuning a Model in order to upload the results to the HuggingFace Hub.</p> <p>You need to:</p> <ul> <li>Create an account: https://huggingface.co/join</li> <li>Follow this guide about <code>User Access Tokens</code></li> </ul>"},{"location":"customization/","title":"\ud83c\udfa8 Customization Guide","text":"<p>This OpenStreetMap AI Helper Blueprint is designed to be flexible and easily adaptable to your specific needs. This guide will walk you through some key areas you can customize to make the Blueprint your own.</p>"},{"location":"customization/#changing-the-model","title":"\ud83e\udde0 Changing the Model","text":"<p>The default provided model is trained to detect swimming pools.</p> <p>You can follow the Create Dataset and Finetune Model.</p> <p>When creating the dataset, you need to pick:</p> <ul> <li>A <code>selector</code> based on OpenStreetMap tags.     The example uses <code>leisure=swimming_pool</code>.     Try to pick elements that can be clearly delimited with a polygon and are easy to distinguish in the satellite images.     A similar example would be a tenis court <code>leisure=pitch + sport=tennis</code>.</li> <li>An appropriate <code>zoom</code> level (the example uses <code>18</code>).     There is a tradeoff between easier detection (higher zoom levels) and covering a wider area on each tile (lower zoom levels).</li> </ul>"},{"location":"customization/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"},{"location":"future-features-contributions/","title":"\ud83d\ude80 Future Features &amp; Contributions","text":"<p>This Blueprint is an evolving project designed to grow with the help of the open-source community. Whether you\u2019re an experienced developer or just starting, there are many ways you can contribute and help shape the future of this tool.</p>"},{"location":"future-features-contributions/#how-you-can-contribute","title":"\ud83c\udf1f How You Can Contribute","text":""},{"location":"future-features-contributions/#enhance-the-blueprint","title":"\ud83d\udee0\ufe0f Enhance the Blueprint","text":"<ul> <li>Check the Issues page to see if there are feature requests you'd like to implement</li> <li>Refer to our Contribution Guide for more details on contributions</li> </ul>"},{"location":"future-features-contributions/#extensibility-ideas","title":"\ud83c\udfa8 Extensibility Ideas","text":"<p>This Blueprint is designed to be a foundation you can build upon. By extending its capabilities, you can open the door to new applications, improve user experience, and adapt the Blueprint to address other use cases. Here are a few ideas for how you can expand its potential:</p> <p>We'd love to see how you can enhance this Blueprint! If you create improvements or extend its capabilities, consider contributing them back to the project so others in the community can benefit from your work. Check out our Contributions Guide to get started!</p>"},{"location":"future-features-contributions/#share-your-ideas","title":"\ud83d\udca1 Share Your Ideas","text":"<p>Got an idea for how this Blueprint could be improved? You can share your suggestions in Discord:</p> <p></p>"},{"location":"future-features-contributions/#build-new-blueprints","title":"\ud83c\udf0d Build New Blueprints","text":"<p>This project is part of a larger initiative to create a collection of reusable starter code solutions that use open-source AI tools. If you\u2019re inspired to create your own Blueprint, you can use the blueprint-template to get started.</p> <p>Your contributions help make this Blueprint better for everyone \ud83c\udf89</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get started with OpenStreetMap AI Helper using one of the options below:</p>"},{"location":"getting-started/#setup-options","title":"Setup options","text":"\u2601\ufe0f Google Colab (GPU)\u2601\ufe0f GitHub Codespaces <p>Get started right away finding swimming pools and contributing them to OpenStreetMap:</p> Find Swimming Pools <p>You can also create your own dataset and finetune a new model for a different use case:</p> Create Dataset Finetune Model <p>Click the button below to launch the project directly in GitHub Codespaces:</p> <p><p></p></p> <p>Once the Codespaces environment launches, inside the terminal, start the Streamlit demo by running:</p> <pre><code>python -m streamlit run demo/app.py\n</code></pre>"},{"location":"step-by-step-guide/","title":"Step-by-Step Guide: How the OpenStreetMap AI Helper Blueprint works","text":"<p>Contributing to OpenStreetMap with the help of AI requires a model trained on an appropriate dataset.</p> <p>We provide tools and example notebooks to:</p> <ul> <li>Creating a custom dataset using ground truth data from OpenStreetMap</li> <li>Finetuning a YOLO detector using the custom dataset</li> </ul> <p>Once you have a trained model, you can Run Inference in order to find the type of elements you used to train the model (in the example, we chose swimming pools).</p>"},{"location":"step-by-step-guide/#overview","title":"Overview","text":"<p>The inference has 4 core stages:</p>"},{"location":"step-by-step-guide/#step-1-pick-a-point-in-the-map-and-download-data-around-it","title":"Step 1: Pick a point in the map and download data around it","text":"<p>After a point is selected, a bounding box is computed around it based on the <code>margin</code> argument.</p> <p>All the existing elements of interest are downloaded from OpenStreetMap using <code>get_elements</code>.</p> <p>All the tiles are downloaded from MapBox using <code>download_stacked_image_and_mask</code>. The elements are grouped and converted to a <code>ground truth mask</code> for later usage.</p>"},{"location":"step-by-step-guide/#step-2-run-inference-on-the-stacked-image","title":"Step 2: Run inference on the stacked image","text":"<p>The stacked image is divided into tiles to run inference using <code>tile_prediction</code> .</p> <p>For each tile, we run the trained YOLO detector.</p> <p>If an object of interest is detected, we pass the bounding box to the provided SAM2 model to obtain a segmentation mask.</p> <p>All the predictions are aggregated into a single <code>stacked output mask</code>.</p>"},{"location":"step-by-step-guide/#step-3-find-existing-new-and-missed-polygons","title":"Step 3: Find existing, new and missed polygons","text":"<p>All the individual mask blobs are converted to polygons for both the <code>stacked output mask</code> and the <code>ground truth mask</code>. Based on overlap, all the polygons are categorized into <code>existing</code>, <code>new</code> or <code>missed</code>.</p>"},{"location":"step-by-step-guide/#step-4-review-filter-and-upload-the-new-polygons-to-openstreetmap","title":"Step 4: Review, filter and upload the <code>new</code> polygons to OpenStreetMap","text":"<p>The <code>new</code> polygons can be manually reviewed, filtered, and then uploaded to OpenStreetMap using <code>upload_osm</code>.</p>"},{"location":"step-by-step-guide/#customizing-the-blueprint","title":"\ud83c\udfa8 Customizing the Blueprint","text":"<p>To better understand how you can tailor this Blueprint to suit your specific needs, please visit the Customization Guide.</p>"},{"location":"step-by-step-guide/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"}]}